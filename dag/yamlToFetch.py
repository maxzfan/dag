"""
Fetch.ai Agent Generator from YAML Configuration

This script reads a YAML configuration file (generated by Claude based on natural language)
and creates a functional Fetch.ai agent from it, including deployment configurations.

Example YAML structure (agent_config.yaml):
---
agent:
  name: "trading_agent"
  seed: "secure_seed_phrase_here"
  port: 8000
  endpoint: "http://localhost:8000/submit"
  
protocols:
  - name: "trading_protocol"
    messages:
      - name: "PriceRequest"
        fields:
          symbol: "str"
          exchange: "str"
      - name: "PriceResponse"
        fields:
          symbol: "str"
          price: "float"
          timestamp: "int"
    
intervals:
  - period: 10.0
    function_name: "check_market"
    description: "Check market conditions every 10 seconds"

storage:
  keys:
    - name: "portfolio"
      default: {}

deployment:
  mode: "production"
  almanac:
    register: true
    network: "testnet"
"""

import yaml
import os
import json
from typing import Dict, Any, List
from pathlib import Path


def generate_imports(config: Dict[str, Any]) -> str:
    """Generate the necessary import statements"""
    imports = ["from uagents import Agent, Context, Model"]
    
    if config.get('protocols'):
        imports.append("from uagents import Protocol")
    
    # Check if LLM integration is needed
    if config.get('integrations', {}).get('llm'):
        llm_provider = config['integrations']['llm'].get('provider', 'openai')
        if llm_provider == 'openai':
            imports.append("import openai")
        elif llm_provider == 'anthropic':
            imports.append("import anthropic")
        elif llm_provider == 'openrouter':
            imports.append("import openai  # OpenRouter uses OpenAI SDK")
    
    imports.extend([
        "from typing import Dict, List, Any",
        "import os",
        "import asyncio",
        "import json",
        "from dotenv import load_dotenv",
        "",
        "# Load environment variables from .env file",
        "load_dotenv()",
    ])
    
    return "\n".join(imports)


def generate_message_models(protocol: Dict[str, Any]) -> str:
    """Generate Pydantic message models from protocol definition"""
    models = []
    
    for message in protocol.get('messages', []):
        model_name = message['name']
        fields = message.get('fields', {})
        
        model_code = f"\nclass {model_name}(Model):\n"
        if fields:
            for field_name, field_type in fields.items():
                model_code += f"    {field_name}: {field_type}\n"
        else:
            model_code += "    pass\n"
        
        models.append(model_code)
    
    return "\n".join(models)


def generate_llm_helper(llm_config: Dict[str, Any]) -> str:
    """Generate LLM integration helper functions"""
    provider = llm_config.get('provider', 'openai')
    model = llm_config.get('model', 'gpt-4')
    base_url = llm_config.get('base_url')
    
    code = f"""
# LLM Integration Helper
class LLMHelper:
    def __init__(self):
        self.provider = "{provider}"
        self.model = "{model}"
        api_key_env = "{llm_config.get('api_key_env', 'OPENAI_API_KEY')}"
        self.api_key = os.getenv(api_key_env)
        
        if not self.api_key:
            raise ValueError(f"Missing API key: {{api_key_env}}")
        
"""
    
    if provider == 'openai':
        code += f"""        openai.api_key = self.api_key
        self.client = openai.OpenAI(api_key=self.api_key)
    
    async def call_llm(self, prompt: str, system_prompt: str = None, context: List[Dict] = None) -> str:
        \"\"\"Call LLM with given prompt\"\"\"
        messages = []
        
        if system_prompt:
            messages.append({{"role": "system", "content": system_prompt}})
        
        if context:
            messages.extend(context)
        
        messages.append({{"role": "user", "content": prompt}})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature={llm_config.get('config', {}).get('temperature', 0.7)},
                max_tokens={llm_config.get('config', {}).get('max_tokens', 1000)}
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error calling LLM: {{str(e)}}"
"""
    elif provider == 'openrouter':
        base_url_str = f'"{base_url}"' if base_url else '"https://openrouter.ai/api/v1"'
        code += f"""        self.base_url = {base_url_str}
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
    
    async def call_llm(self, prompt: str, system_prompt: str = None, context: List[Dict] = None) -> str:
        \"\"\"Call LLM via OpenRouter with given prompt\"\"\"
        messages = []
        
        if system_prompt:
            messages.append({{"role": "system", "content": system_prompt}})
        
        if context:
            messages.extend(context)
        
        messages.append({{"role": "user", "content": prompt}})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature={llm_config.get('config', {}).get('temperature', 0.7)},
                max_tokens={llm_config.get('config', {}).get('max_tokens', 1000)},
                extra_headers={{
                    "HTTP-Referer": "https://fetch.ai",  # Optional: Replace with your site
                    "X-Title": "Fetch.ai Agent"  # Optional: Replace with your app name
                }}
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error calling LLM via OpenRouter: {{str(e)}}"
"""
    elif provider == 'anthropic':
        code += f"""        self.client = anthropic.Anthropic(api_key=self.api_key)
    
    async def call_llm(self, prompt: str, system_prompt: str = None, context: List[Dict] = None) -> str:
        \"\"\"Call LLM with given prompt\"\"\"
        messages = []
        
        if context:
            messages.extend(context)
        
        messages.append({{"role": "user", "content": prompt}})
        
        try:
            response = self.client.messages.create(
                model=self.model,
                system=system_prompt or "You are a helpful AI assistant.",
                messages=messages,
                max_tokens={llm_config.get('config', {}).get('max_tokens', 1000)},
                temperature={llm_config.get('config', {}).get('temperature', 0.7)}
            )
            return response.content[0].text
        except Exception as e:
            return f"Error calling LLM: {{str(e)}}"
"""
    
    code += "\nllm_helper = LLMHelper()\n"
    return code


def generate_protocol_handlers(protocol: Dict[str, Any], has_llm: bool = False) -> str:
    """Generate protocol message handlers"""
    protocol_name = protocol['name']
    handlers = [f'\n{protocol_name} = Protocol("{protocol_name}")\n']
    
    for message in protocol.get('messages', []):
        msg_name = message['name']
        
        # Generate handler for Request messages
        if 'Request' in msg_name:
            response_name = msg_name.replace('Request', 'Response')
            
            # Check if this is an LLM request
            is_llm_request = 'LLM' in msg_name
            
            handler_code = f"""
@{protocol_name}.on_message(model={msg_name})
async def handle_{msg_name.lower()}(ctx: Context, sender: str, msg: {msg_name}):
    ctx.logger.info(f"Received {{msg.__class__.__name__}} from {{sender}}")
    
"""
            if is_llm_request and has_llm:
                handler_code += """    # Process LLM request
    import time
    try:
        system_prompt = msg.system_prompt if hasattr(msg, 'system_prompt') and msg.system_prompt else None
        context = msg.context if hasattr(msg, 'context') and msg.context else None
        
        ctx.logger.info(f"Calling LLM with prompt: {{msg.prompt[:100]}}...")
        llm_response = await llm_helper.call_llm(
            prompt=msg.prompt,
            system_prompt=system_prompt,
            context=context
        )
        
        ctx.logger.info(f"LLM Response received: {{len(llm_response)}} characters")
        
        # Send LLM response back
        await ctx.send(sender, LLMResponse(
            request_id=msg.request_id,
            response=llm_response,
            tool_calls=[],
            tokens_used=0,  # TODO: Track actual token usage
            finish_reason="stop",
            timestamp=int(time.time())
        ))
        ctx.logger.info(f"Sent LLM response to {{sender}}")
    except Exception as e:
        ctx.logger.error(f"Error processing LLM request: {{e}}")
        await ctx.send(sender, LLMResponse(
            request_id=msg.request_id,
            response=f"Error: {{str(e)}}",
            tool_calls=[],
            tokens_used=0,
            finish_reason="error",
            timestamp=int(time.time())
        ))
"""
            else:
                # Generate a working example response with all required fields
                fields = message.get('fields', {})
                response_fields = []
                
                handler_code += """    # TODO: Implement your logic here
    # This is a mock response - replace with your actual logic
    import time
    
"""
                
                # Try to find the corresponding response message to get its fields
                response_msg = next((m for m in protocol.get('messages', []) if m['name'] == response_name), None)
                if response_msg and response_msg.get('fields'):
                    for field_name, field_type in response_msg['fields'].items():
                        # Generate sensible default values based on type
                        if field_type == 'str':
                            if 'id' in field_name.lower():
                                response_fields.append(f'        {field_name}=msg.{field_name} if hasattr(msg, "{field_name}") else "generated_id",')
                            else:
                                response_fields.append(f'        {field_name}="mock_{field_name}",')
                        elif field_type == 'int':
                            if 'time' in field_name.lower():
                                response_fields.append(f'        {field_name}=int(time.time()),')
                            else:
                                response_fields.append(f'        {field_name}=0,')
                        elif field_type == 'float':
                            response_fields.append(f'        {field_name}=0.0,')
                        elif field_type == 'bool':
                            response_fields.append(f'        {field_name}=True,')
                        else:
                            response_fields.append(f'        {field_name}={{}},')
                    
                    handler_code += f"""    response = {response_name}(
{chr(10).join(response_fields)}
    )
    
    ctx.logger.info(f"Sending {{response.__class__.__name__}} to {{sender}}")
    await ctx.send(sender, response)
"""
                else:
                    handler_code += f"""    # Example response - update with actual fields
    response = {response_name}()
    await ctx.send(sender, response)
"""
            
            handlers.append(handler_code)
    
    return "\n".join(handlers)


def generate_interval_tasks(intervals: List[Dict[str, Any]]) -> str:
    """Generate periodic interval tasks"""
    tasks = []
    
    for interval in intervals:
        period = interval['period']
        func_name = interval['function_name']
        description = interval.get('description', '')
        uses_llm = interval.get('uses_llm', False)
        custom_impl = interval.get('custom_implementation', '')
        
        task_code = f"""
@agent.on_interval(period={period})
async def {func_name}(ctx: Context):
    \"\"\"
    {description}
    \"\"\"
    ctx.logger.info("Running {func_name}...")
    
"""
        
        # If there's custom implementation, use it directly
        if custom_impl:
            # Indent the custom implementation properly
            indented_custom = '\n'.join('    ' + line if line.strip() else '' 
                                       for line in custom_impl.strip().split('\n'))
            task_code += indented_custom + "\n"
        elif uses_llm:
            task_code += """    # Example: Process LLM queue
    # queue = ctx.storage.get("llm_queue", [])
    # for item in queue:
    #     response = await llm_helper.call_llm(item["prompt"])
    #     # Process response
    
"""
        else:
            task_code += """    # TODO: Implement your periodic task logic here
    # Access storage: ctx.storage.get("key")
    # Update storage: ctx.storage.set("key", value)
"""
        
        tasks.append(task_code)
    
    return "\n".join(tasks)


def generate_startup_handler(config: Dict[str, Any]) -> str:
    """Generate startup event handler"""
    storage_config = config.get('storage', {})
    storage_keys = storage_config.get('keys', [])
    startup_config = config.get('startup', {})
    startup_tasks = startup_config.get('tasks', [])
    custom_logging = startup_config.get('custom_logging', '')
    
    code = """
@agent.on_event("startup")
async def startup(ctx: Context):
"""
    
    # If there's custom logging, use it
    if custom_logging:
        indented_custom = '\n'.join('    ' + line if line.strip() else '' 
                                   for line in custom_logging.strip().split('\n'))
        code += indented_custom + "\n"
    else:
        code += """    ctx.logger.info(f"Agent {agent.name} starting up...")
    ctx.logger.info(f"Agent address: {agent.address}")
"""
    
    if storage_keys:
        code += "\n    # Initialize storage\n"
        for key_config in storage_keys:
            key_name = key_config['name']
            default = key_config.get('default', {})
            default_str = json.dumps(default) if isinstance(default, (dict, list)) else repr(default)
            code += f'    ctx.storage.set("{key_name}", {default_str})\n'
    
    if startup_tasks and not custom_logging:
        code += "\n    # Startup tasks\n"
        for task in startup_tasks:
            code += f'    ctx.logger.info("TODO: {task}")\n'
    
    # Add deployment-specific startup (only if no custom logging)
    if not custom_logging:
        deployment = config.get('deployment', {})
        if deployment.get('almanac', {}).get('register'):
            code += '\n    # Agent is configured for Almanac registration\n'
            code += f'    ctx.logger.info("Almanac network: {deployment["almanac"].get("network", "testnet")}")\n'
        
        if deployment.get('mailbox', {}).get('enabled'):
            code += '\n    # Agent is configured with Mailbox\n'
            code += '    ctx.logger.info("Mailbox enabled for remote communication")\n'
    
    return code


def generate_deployment_config(config: Dict[str, Any], output_dir: str = "."):
    """Generate deployment configuration files"""
    deployment = config.get('deployment', {})
    
    if not deployment:
        return
    
    # Generate .env file
    env_vars = []
    env_vars.append(f"AGENT_SEED={config['agent'].get('seed', 'your_seed_here')}")
    
    if deployment.get('almanac', {}).get('network'):
        env_vars.append(f"FETCH_NETWORK={deployment['almanac']['network']}")
    
    if deployment.get('mailbox', {}).get('key'):
        env_vars.append(f"AGENT_MAILBOX_KEY={deployment['mailbox']['key']}")
    
    # LLM API keys
    if config.get('integrations', {}).get('llm'):
        llm_config = config['integrations']['llm']
        api_key_env = llm_config.get('api_key_env', 'OPENAI_API_KEY')
        env_vars.append(f"{api_key_env}=your_api_key_here")
        
        # Add base_url hint for OpenRouter
        if llm_config.get('provider') == 'openrouter':
            env_vars.append("# OpenRouter base URL: https://openrouter.ai/api/v1")
    
    # Write .env file
    env_path = os.path.join(output_dir, ".env")
    with open(env_path, 'w') as f:
        f.write("\n".join(env_vars))
    
    print(f"[OK] Generated .env file: {env_path}")
    
    # Generate deployment README
    readme_content = f"""# {config['agent']['name']} Deployment Guide

## Prerequisites
- Python 3.8+
- uagents library: `pip install uagents`
"""
    
    if config.get('integrations', {}).get('llm'):
        provider = config['integrations']['llm'].get('provider')
        readme_content += f"- {provider} library: `pip install "
        if provider == 'openrouter':
            readme_content += "openai  # OpenRouter uses OpenAI SDK`\n"
        elif provider == 'anthropic':
            readme_content += "anthropic`\n"
        else:
            readme_content += f"{provider}`\n"
    
    readme_content += """
## Setup

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure environment variables:**
   ```bash
   # Edit .env with your actual values
   ```

3. **Test locally:**
   ```bash
   python generated_agent.py
   ```

## Deployment Options

"""
    
    hosting_type = deployment.get('hosting', {}).get('type', 'self_hosted')
    
    if hosting_type == 'self_hosted':
        readme_content += """### Self-Hosted Deployment

1. **Set up your server:**
   - Ensure port {} is open
   - Configure firewall rules
   - Set up reverse proxy (nginx/caddy) if using SSL

2. **Run as systemd service:**
   ```bash
   sudo cp fetch-agent.service /etc/systemd/system/
   sudo systemctl enable fetch-agent
   sudo systemctl start fetch-agent
   ```

3. **Check status:**
   ```bash
   sudo systemctl status fetch-agent
   ```
""".format(config['agent'].get('port', 8000))
    
    elif hosting_type == 'agentverse':
        readme_content += """### Agentverse Deployment

1. **Login to Agentverse:**
   Visit https://agentverse.ai

2. **Upload agent code:**
   - Create new agent
   - Copy code from generated_agent.py
   - Configure environment variables

3. **Deploy:**
   - Click "Deploy"
   - Agent will be automatically hosted and registered
"""
    
    if deployment.get('almanac', {}).get('register'):
        readme_content += f"""
## Almanac Registration

This agent is configured to register with the Fetch.ai Almanac on **{deployment['almanac'].get('network', 'testnet')}**.

The agent will be discoverable by other agents on the network.

"""
    
    if deployment.get('discovery', {}).get('deltav', {}).get('enabled'):
        readme_content += """
## DeltaV Integration

This agent is configured for DeltaV integration, making it accessible to end-users through natural language interfaces.

Users can interact with your agent through:
- DeltaV mobile app
- DeltaV web interface
- Voice assistants integrated with Fetch.ai

"""
    
    readme_path = os.path.join(output_dir, "DEPLOYMENT.md")
    with open(readme_path, 'w') as f:
        f.write(readme_content)
    
    print(f"[OK] Generated deployment guide: {readme_path}")
    
    # Generate systemd service file for self-hosted
    if hosting_type == 'self_hosted':
        service_content = f"""[Unit]
Description=Fetch.ai Agent - {config['agent']['name']}
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/agent
Environment="PATH=/home/ubuntu/.local/bin:/usr/local/bin:/usr/bin:/bin"
EnvironmentFile=/home/ubuntu/agent/.env
ExecStart=/usr/bin/python3 generated_agent.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
"""
        service_path = os.path.join(output_dir, "fetch-agent.service")
        with open(service_path, 'w') as f:
            f.write(service_content)
        
        print(f"[OK] Generated systemd service file: {service_path}")
    
    # Generate requirements.txt
    requirements = ["uagents>=0.12.0", "python-dotenv>=1.0.0"]
    
    # Check if requests is needed (for API calls in custom implementations)
    needs_requests = False
    if config.get('intervals'):
        for interval in config['intervals']:
            if 'requests' in interval.get('custom_implementation', ''):
                needs_requests = True
                break
    
    if needs_requests:
        requirements.append("requests>=2.31.0")
    
    if config.get('integrations', {}).get('llm'):
        provider = config['integrations']['llm'].get('provider')
        if provider == 'openai':
            requirements.append("openai>=1.0.0")
        elif provider == 'anthropic':
            requirements.append("anthropic>=0.18.0")
        elif provider == 'openrouter':
            requirements.append("openai>=1.0.0  # OpenRouter uses OpenAI SDK")
    
    requirements_path = os.path.join(output_dir, "requirements.txt")
    with open(requirements_path, 'w') as f:
        f.write("\n".join(requirements))
    
    print(f"[OK] Generated requirements.txt: {requirements_path}")


def generate_agent_from_yaml(yaml_path: str, output_path: str = "generated_agent.py", output_dir: str = "."):
    """
    Main function to generate a Fetch.ai agent from YAML configuration
    
    Args:
        yaml_path: Path to the YAML configuration file
        output_path: Path where the generated Python agent will be saved
        output_dir: Directory for all generated files
    """
    
    # Load YAML configuration
    with open(yaml_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Start building the agent code
    code_parts = []
    
    # 1. Imports
    code_parts.append(generate_imports(config))
    code_parts.append("\nimport time\n")
    
    # 2. LLM Helper (if needed)
    has_llm = bool(config.get('integrations', {}).get('llm'))
    if has_llm:
        code_parts.append(generate_llm_helper(config['integrations']['llm']))
    
    # 3. Create agent instance
    agent_config = config['agent']
    
    # Handle mailbox configuration
    mailbox_key = None
    if config.get('deployment', {}).get('mailbox', {}).get('enabled'):
        mailbox_key = config['deployment']['mailbox'].get('key')
        if mailbox_key:
            mailbox_key = f'"{mailbox_key}"'
        else:
            mailbox_key = 'os.getenv("AGENT_MAILBOX_KEY")'
    
    agent_def = f"""
# Create the agent
agent = Agent(
    name="{agent_config['name']}",
    seed="{agent_config.get('seed', agent_config['name'] + '_seed')}",
    port={agent_config.get('port', 8000)},
    endpoint=["{agent_config.get('endpoint', 'http://localhost:8000/submit')}"],"""
    
    if mailbox_key:
        agent_def += f"\n    mailbox={mailbox_key},"
    
    agent_def += "\n)\n"
    code_parts.append(agent_def)
    
    # 4. Generate message models and protocols
    if config.get('protocols'):
        for protocol in config['protocols']:
            code_parts.append(generate_message_models(protocol))
            code_parts.append(generate_protocol_handlers(protocol, has_llm))
            code_parts.append(f"\nagent.include({protocol['name']})\n")
    
    # 5. Generate interval tasks
    if config.get('intervals'):
        code_parts.append(generate_interval_tasks(config['intervals']))
    
    # 6. Generate startup handler
    code_parts.append(generate_startup_handler(config))
    
    # 7. Add main runner
    main_code = """

if __name__ == "__main__":
    agent.run()
"""
    code_parts.append(main_code)
    
    # Write agent code to file
    full_code = "".join(code_parts)
    
    output_file_path = os.path.join(output_dir, output_path)
    with open(output_file_path, 'w') as f:
        f.write(full_code)
    
    print(f"[OK] Agent generated successfully: {output_file_path}")
    print(f"   Agent name: {agent_config['name']}")
    print(f"   Agent address will be generated from seed")
    
    # Generate deployment files
    generate_deployment_config(config, output_dir)
    
    print(f"\n[INFO] Deployment mode: {config.get('deployment', {}).get('mode', 'local')}")
    print(f"   Run with: python {output_file_path}")
    
    return full_code


# Example usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        yaml_file = sys.argv[1]
        output_name = sys.argv[2] if len(sys.argv) > 2 else "generated_agent.py"
        output_directory = sys.argv[3] if len(sys.argv) > 3 else "./agent_output"
        
        generate_agent_from_yaml(yaml_file, output_name, output_directory)
    else:
        # Example: Create a sample YAML config with deployment
        sample_config = """
agent:
  name: "llm_weather_agent"
  seed: "llm_weather_agent_seed_123"
  port: 8001
  endpoint: "http://localhost:8001/submit"

integrations:
  llm:
    provider: "openrouter"
    model: "anthropic/claude-3.5-sonnet"
    api_key_env: "OPENROUTER_API_KEY"
    base_url: "https://openrouter.ai/api/v1"
    config:
      temperature: 0.7
      max_tokens: 500
    system_prompts:
      default: "You are a helpful weather assistant that provides accurate weather information."
  
protocols:
  - name: "weather_protocol"
    messages:
      - name: "WeatherRequest"
        fields:
          location: "str"
          units: "str"
      - name: "WeatherResponse"
        fields:
          location: "str"
          temperature: "float"
          conditions: "str"
          timestamp: "int"
      - name: "LLMRequest"
        fields:
          request_id: "str"
          prompt: "str"
          context: "List[Dict[str, str]]"
          system_prompt: "str"
      - name: "LLMResponse"
        fields:
          request_id: "str"
          response: "str"
          tool_calls: "List[Dict[str, Any]]"
          tokens_used: "int"
          finish_reason: "str"
          timestamp: "int"
    
intervals:
  - period: 300.0
    function_name: "update_weather_cache"
    description: "Update weather data cache every 5 minutes"
  
  - period: 30.0
    function_name: "process_llm_queue"
    description: "Process queued LLM requests"
    uses_llm: true

storage:
  keys:
    - name: "weather_cache"
      default: {}
    - name: "request_log"
      default: []
    - name: "llm_queue"
      default: []

startup:
  tasks:
    - "Connect to weather API"
    - "Load cached weather data"
    - "Initialize LLM client"

deployment:
  mode: "production"
  
  almanac:
    register: true
    network: "testnet"
    service_endpoints:
      - protocol: "weather_protocol"
        endpoint: "http://your-server.com:8001/submit"
    service_metadata:
      name: "LLM Weather Agent"
      description: "AI-powered weather information service"
      category: "data_oracle"
      tags:
        - "weather"
        - "llm"
        - "ai"
  
  mailbox:
    enabled: true
    key: "your-mailbox-key-here"
  
  hosting:
    type: "self_hosted"
    self_hosted:
      server_ip: "your-server-ip"
      ssl_enabled: true
  
  discovery:
    deltav:
      enabled: true
      function_groups:
        - name: "Weather Functions"
          description: "Get weather information with AI assistance"
          functions:
            - name: "get_weather"
              description: "Get current weather with natural language"
              parameters:
                - name: "query"
                  type: "string"
                  required: true
                  description: "Natural language weather query"
      pricing:
        model: "free"
"""
        
        # Save sample config
        with open("sample_llm_agent_config.yaml", "w") as f:
            f.write(sample_config)
        
        print("Sample YAML config created: sample_llm_agent_config.yaml")
        print("\nGenerating agent with deployment configs...\n")
        
        # Generate the agent
        generate_agent_from_yaml("weather_aggregator.yaml", "weather_aggregator.py", "./weather_output")